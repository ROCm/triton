From b45bde0476a4bd3d984732217146611fe4c489d3 Mon Sep 17 00:00:00 2001
From: Michael Melesse <micmelesse@gmail.com>
Date: Mon, 11 Sep 2023 13:11:47 -0500
Subject: [PATCH] Upstream patch for Gen Backend

This is a combination of 3 commits.

Use Gen Backend with Dots working

Use Gen Backend

This is a combination of 2 commits.

Use Gen Backend

This is a combination of 2 commits.

Use Gen Backend

This is a combination of 2 commits.

Use Gen Backend

Upstream state when Gen Backend Works

This is a combination of 4 commits.

This is a combination of 2 commits.

Upstream state when Gen Backend Works

This is a combination of 29 commits.

Upstream is already to far ahead.

This is a combination of 7 commits.

Standalone(All) and Backend(test_empty) Works

Standalone(All) and Backend(test_empty) Works

This is a combination of 4 commits.

Backend works as Standalone as well

This is a combination of 12 commits.

add dockerfile

pull third_party_backend_to_merge

add scripts

update amd_hip_backend

fix tests

save changes

empty kernel works

upate to rebased empty kernel working branch

backend dir with empty kernel working

set option to build backend mode

works on both upstream and downstream

Backend works as Standalone as well

backend works again

clean upstream location first before copy

compressed commit

clean up a bit

use gen branch

add scripts

remove gemm scripts

run with python function

gen backend when runing

update backend commit

install packages when genning backend

fix gen backend script

send every info to backend

gen quietly

install tqdm

get in the fork

add compiler.py

add gened backend

copy and chmod 777

update run script

update gened backend

not print on parse

update gened backend

cleaner compiler

working imports

working!

cleaner test run

more comment out

skip segfaults

match fork

test_convert2d works

unskip a bunch of stuff

fix shift ops

enable test_atomic_rmw

remove dot skips

min diff gen backend

update gened backend

add clean cache function

min diff

use new gen_backend branch

update gened files

update backend

try minimal backend

update to squashed commit

update triton-tot

update backend

clean

min diff

add unit test fixes

ignore argmax in general

update backend
---
 python/test/unit/language/test_core.py | 74 ++++++++++++-----------
 python/triton/compiler/compiler.py     | 23 ++++++--
 python/triton/language/core.py         |  6 ++
 python/triton/language/semantic.py     | 81 +++++++++++++++++++++-----
 third_party/amd_hip_backend            |  2 +-
 5 files changed, 131 insertions(+), 55 deletions(-)

diff --git a/python/test/unit/language/test_core.py b/python/test/unit/language/test_core.py
index 1c92ef689..163f303b8 100644
--- a/python/test/unit/language/test_core.py
+++ b/python/test/unit/language/test_core.py
@@ -25,11 +25,10 @@ torch_dtypes = ['bool'] + int_dtypes + ['uint8'] + float_dtypes + ['bfloat16']
 # num_ctas_list = [1, 4] if torch.cuda.get_device_capability()[0] == 9 else [1]
 num_ctas_list = [1]
 
+GPU_DIALECT = "triton_gpu"
 if is_hip():
-    GPU_DIALECT = "triton_gpu_rocm"
     THREADS_PER_WARP = 64
 else:
-    GPU_DIALECT = "triton_gpu"
     THREADS_PER_WARP = 32
 
 
@@ -1018,8 +1017,8 @@ def noinline_multi_values_fn(x, y, Z):
 
 @pytest.mark.parametrize("mode", ["simple", "call_graph", "shared", "dynamic", "multi_values"])
 def test_noinline(mode, device):
-    if is_hip() and mode == "shared":
-        pytest.skip('test_noinline["shared"] not supported on HIP.')
+    if is_hip():
+        pytest.skip('test_noinline not supported on HIP.')
 
     @triton.jit
     def kernel(X, Y, Z):
@@ -1075,6 +1074,8 @@ def test_noinline(mode, device):
                                    for sem in [None, 'acquire', 'release', 'acq_rel', 'relaxed']]))
 def test_atomic_rmw(op, dtype_x_str, mode, sem, device):
     check_cuda_only(device)
+    if is_hip():
+        pytest.skip(f'test_atomic_rmw currently not supported on HIP.')
 
     capability = torch.cuda.get_device_capability()
     if capability[0] < 7:
@@ -1548,7 +1549,7 @@ def get_reduced_dtype(dtype_str, op):
 def test_reduce1d(op, dtype_str, shape, num_ctas, device):
     check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested
 
-    if is_hip():
+    if is_hip() and (op == "min-with-indices" or op == "max-with-indices") and shape>=128:
         pytest.skip("test_reduce1d not supported on HIP")
 
     # triton kernel
@@ -1640,11 +1641,10 @@ reduce_configs3 = [(op, 'float32', shape, axis)
 @pytest.mark.parametrize("op, dtype_str, shape, axis", reduce_configs1 + reduce_configs2 + reduce_configs3)
 @pytest.mark.parametrize("num_ctas", num_ctas_list)
 def test_reduce(op, dtype_str, shape, axis, num_ctas, device):
-    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested
 
-    if is_hip():
-        pytest.skip("test_reduce2d not supported on HIP")
-    # triton kernel
+    if is_hip() and op == "argmax":
+        pytest.skip("test_reduce not supported on HIP")
+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested
 
     @triton.jit
     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, IS_3D: tl.constexpr,
@@ -1736,8 +1736,6 @@ def get_first_element(a, b):
 
 @pytest.mark.parametrize("op, dtype_str, shape, axis, num_warps", scan_configs)
 def test_scan2d(op, dtype_str, shape, axis, num_warps, device):
-    if is_hip():
-        pytest.skip("test_scan2d is not supported in HIP")
     check_type_supported(dtype_str, device)
 
     # triton kernel
@@ -1854,7 +1852,7 @@ def test_scan_layouts(M, N, src_layout, axis, device):
 
     ir = f"""
     #blocked = {src_layout}
-    module attributes {{"triton_gpu.num-warps" = 4 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.threads-per-warp" = 32 : i32}} {{
+    module attributes {{"triton_gpu.num-warps" = 4 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.threads-per-warp" = {THREADS_PER_WARP} : i32}} {{
     tt.func public @kernel_0d1d(%arg0: !tt.ptr<i32, 1> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32, 1> {{tt.divisibility = 16 : i32}}) {{
       %cst = arith.constant dense<{N}> : tensor<{M}x1xi32, #blocked>
       %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>
@@ -1922,8 +1920,8 @@ layouts = [
 @pytest.mark.parametrize("dtype_str", ["int32", "float32", "float16"])
 @pytest.mark.parametrize("reduce_op", ["sum", "max"])
 def test_reduce_layouts(M, N, src_layout, axis, reduce2d, dtype_str, reduce_op, device):
-    if is_hip():
-        pytest.skip("test_reduce_layouts is not supported in HIP")
+    if is_hip() and "triton_gpu.mma" in str(src_layout) and src_layout.instr_shape == str([16,16,16]):
+        pytest.skip(f'test_reduce_layouts{src_layout} currently not supported on HIP.')
     if reduce_op == "sum" and dtype_str == "float16" and M * N > 1024:
         pytest.skip("Skipping sum reduction on float16 due to accuracy issues")
 
@@ -1961,7 +1959,7 @@ def test_reduce_layouts(M, N, src_layout, axis, reduce2d, dtype_str, reduce_op,
     ir = f"""
     #blocked = {blocked}
     #src = {src_layout}
-    module attributes {{"triton_gpu.num-warps" = 4 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.threads-per-warp" = 32 : i32}} {{
+    module attributes {{"triton_gpu.num-warps" = 4 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.threads-per-warp" = {THREADS_PER_WARP} : i32}} {{
     tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<{ty}, 1> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<{ty}, 1> {{tt.divisibility = 16 : i32}}) {{
         %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #blocked}}>>
         %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #{GPU_DIALECT}.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>
@@ -2017,9 +2015,6 @@ layouts = [
 @pytest.mark.parametrize("M", [32, 64, 128, 256])
 @pytest.mark.parametrize("src_layout", layouts)
 def test_store_op(M, src_layout, device):
-    if is_hip():
-        pytest.skip("test_convert1d is not supported yet in HIP")
-
     ir = f"""
     #src = {src_layout}
     module attributes {{"{GPU_DIALECT}.num-warps" = 4 : i32, "{GPU_DIALECT}.num-ctas" = 1 : i32, "{GPU_DIALECT}.threads-per-warp" = {THREADS_PER_WARP} : i32}} {{
@@ -2070,8 +2065,6 @@ layouts = [
 @pytest.mark.parametrize("src_dim", [0, 1])
 @pytest.mark.parametrize("dst_dim", [0, 1])
 def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):
-    if is_hip():
-        pytest.skip("test_convert1d is not supported in HIP")
 
     ir = f"""
     #dst = {dst_layout}
@@ -2132,9 +2125,6 @@ layouts = [
 @pytest.mark.parametrize("op", ["sum", "max"])
 @pytest.mark.parametrize("first_axis", [0, 1])
 def test_chain_reduce(M, N, src_layout, op, device, first_axis):
-    if is_hip():
-        pytest.skip("test_chain_reduce is not supported in HIP")
-
     op_str = ""
     if op == "sum":
         op_str = """
@@ -2198,6 +2188,8 @@ def test_chain_reduce(M, N, src_layout, op, device, first_axis):
 
 
 def test_generic_reduction(device):
+    if is_hip():
+        pytest.skip("test_generic_reduction not supported on HIP")
 
     @triton.jit
     def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):
@@ -2313,14 +2305,29 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o
     capability = torch.cuda.get_device_capability()
 
     if is_hip():
+        # TODO consider reenabling this tests when fp8 casing is fixed
+        if M == 16 and N == 16 and K == 16 and "float8" in in_dtype:
+            pytest.skip("triton do not generate MFMA instructions for given block size")
+
         # set capability to large number to jump over check below
         # check are not relevant to amd gpu, left them for smaller diff between test_core.py and test_core_amd.py tests
+        if (M, N, K) == (128, 256, 32):
+            pytest.skip("Out of resources")
         capability = (100, 100)
         if out_dtype is None:
             if in_dtype in float_dtypes:
                 out_dtype = "float32"
             else:
                 out_dtype = "int32"
+        if (M, N, K) in [(64, 128, 128)]:
+            pytest.skip(f"test_dot{(M, N, K)} not supported on HIP: memory out of resource.")
+        if (M, N, K, num_warps) in [(128, 256, 32, 8), (128, 128, 64, 4)]:
+            pytest.skip(f"test_dot{(M, N, K)} not supported on HIP. Reduce Warp to work")
+        if M == 16 or N == 16 or K == 16:
+            pytest.skip(f"test_dot{(M, N, K)} segfaults on HIP")
+        if epilogue == "softmax":
+            pytest.skip(f"test_dot{epilogue} segfaults on HIP")
+
 
     if capability[0] < 7:
         pytest.skip("Only test tl.dot() on devices with sm >= 70")
@@ -2336,15 +2343,6 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o
             # TODO: support out_dtype=float16 for tl.dot on V100
             pytest.skip("Only test out_dtype=float16 on devices with sm >=80")
 
-    if is_hip():
-        if (M, N, K) in [(64, 128, 128)]:
-            pytest.skip(f"test_dot{(M, N, K)} not supported on HIP: memory out of resource.")
-        if (M, N, K, num_warps) in [(128, 256, 32, 8), (128, 128, 64, 4)]:
-            pytest.skip(f"test_dot{(M, N, K)} not supported on HIP. Reduce Warp to work")
-        if M == 16 or N == 16 or K == 16:
-            pytest.skip(f"test_dot{(M, N, K)} segfaults on HIP")
-        if epilogue == "softmax":
-            pytest.skip(f"test_dot{epilogue} segfaults on HIP")
 
     torch.backends.cuda.matmul.allow_tf32 = allow_tf32
 
@@ -3469,6 +3467,8 @@ def add_fn_static_cond(x, cond: tl.constexpr):
     "call_type",
     ["attribute", "attribute_jit", "jit", "jit_if", "jit_expr", "jit_static_cond", "jit_noinline", "jit_extern"])
 def test_if_call(call_type, device):
+    if is_hip():
+        pytest.skip("test_if_call not supported on HIP")
 
     @triton.jit
     def kernel(Out, call_type: tl.constexpr):
@@ -3729,14 +3729,14 @@ intermediate_layouts = [
 @pytest.mark.parametrize("interm_layout", intermediate_layouts)
 @pytest.mark.parametrize("dst_layout", layouts)
 def test_convert2d(M, N, src_layout, interm_layout, dst_layout, dtype, device):
-    if is_hip():
-        pytest.skip("test_convert2d is not supported in HIP")
     if (M == 1 or N == 1) and interm_layout:
         pytest.skip("Out of bound access when maxPhase > 1")
     if str(src_layout) == str(dst_layout):
         pytest.skip()
     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):
         pytest.skip()
+    if is_hip() and M == 128 and N == 128:
+        pytest.skip(f'test_convert2d currently  out of resource not supported on HIP.')
 
     layouts = f"""
     #src = {src_layout}
@@ -3761,7 +3761,7 @@ def test_convert2d(M, N, src_layout, interm_layout, dst_layout, dtype, device):
     """
 
     ir = layouts + f"""
-    module attributes {{"triton_gpu.num-warps" = 4 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.threads-per-warp" = 32 : i32}} {{
+    module attributes {{"triton_gpu.num-warps" = 4 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.threads-per-warp" = {THREADS_PER_WARP} : i32}} {{
   tt.func public @kernel_0d1d(%arg0: !tt.ptr<f16, 1> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<f16, 1> {{tt.divisibility = 16 : i32}}) {{
     %cst = arith.constant dense<{N}> : tensor<{M}x1xi32, #src>
     %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>
@@ -3910,6 +3910,8 @@ def matmul_kernel(  #
 @pytest.mark.parametrize("in_type_str", ['float8e5', 'float8e4nv'])
 @pytest.mark.parametrize("low_precision_acc", [0, 32, 64, 128])
 def test_fp8_dot_acc(in_type_str, low_precision_acc, device):
+    if is_hip():
+        pytest.skip(f'test_fp8_dot_acc skipped on hip')
     check_type_supported(in_type_str, device)
     M, N, K = 128, 256, 256
     BLOCK_M, BLOCK_N, BLOCK_K = 128, 256, 128
@@ -3943,6 +3945,8 @@ def test_fp8_dot_acc(in_type_str, low_precision_acc, device):
 
 @pytest.mark.parametrize("enable_fp_fusion", [False, True])
 def test_enable_fp_fusion(enable_fp_fusion):
+    if is_hip():
+        pytest.skip(f'test_enable_fp_fusion skipped on hip')
     # Sequential multiply add can be fused by backend
     @triton.jit
     def mul_add(data):
diff --git a/python/triton/compiler/compiler.py b/python/triton/compiler/compiler.py
index 9ecd15d71..1b5c64283 100644
--- a/python/triton/compiler/compiler.py
+++ b/python/triton/compiler/compiler.py
@@ -292,10 +292,7 @@ arg_type_pattern = {
     "ttgir": mlir_arg_type_pattern,
     "ptx": ptx_arg_type_pattern,
 }
-if is_hip():
-    ttgir_num_warps_pattern = r'"triton_gpu_rocm.num-warps"\s?=\s?(\d+)\s?:'
-else:
-    ttgir_num_warps_pattern = r'"triton_gpu.num-warps"\s?=\s?(\d+)\s?:'
+ttgir_num_warps_pattern = r'"triton_gpu.num-warps"\s?=\s?(\d+)\s?:'
 
 
 def _get_jsonable_constants(constants):
@@ -422,7 +419,23 @@ def compile(fn, **kwargs):
                           lambda src: ttgir_to_llir(src, extern_libs, target, tma_infos))
         add_cuda_stages(target, extern_libs, stages)
     elif device_type == "hip":
-        _device_backend.add_stages(target, extern_libs, stages, num_warps=num_warps, num_stages=num_stages)
+        # pass the user's configuration to the backend device.
+        target["num_warps"] = num_warps
+        target["num_stages"] = num_stages
+        target["num_ctas"] = num_ctas
+
+        other = {}
+        other["context"] = context
+        other["warp_size"] = target["warp_size"]
+        other["cluster_info"] = cluster_info 
+        other["enable_warp_specialization"] = enable_warp_specialization
+        other["enable_persistent"] = enable_persistent
+        other["optimize_epilogue"] = optimize_epilogue 
+        other["tma_infos"] = tma_infos
+        other["waves_per_eu"] = 0
+        other["matrix_instr_nonkdim"] = 0
+        
+        _device_backend.add_stages(target, extern_libs, stages, other) 
     else:
         # pass the user's configuration to the backend device.
         target["num_warps"] = num_warps
diff --git a/python/triton/language/core.py b/python/triton/language/core.py
index 2919689a0..fdd5bb9c1 100644
--- a/python/triton/language/core.py
+++ b/python/triton/language/core.py
@@ -145,6 +145,12 @@ class dtype:
     def is_fp8e5(self):
         return self.name == 'fp8e5'
 
+    def is_fp8e4b8(self):
+        return self.name == 'fp8e4b8'
+    
+    def is_fp8e5b16(self):
+        return self.name == 'fp8e5b16'
+
     def is_fp16(self):
         return self.name == 'fp16'
 
diff --git a/python/triton/language/semantic.py b/python/triton/language/semantic.py
index d74cbb150..dce01471b 100644
--- a/python/triton/language/semantic.py
+++ b/python/triton/language/semantic.py
@@ -1172,16 +1172,24 @@ def atomic_xchg(ptr: tl.tensor, val: tl.tensor, mask: tl.tensor, sem: str, scope
 # ===----------------------------------------------------------------------===//
 
 
-def gpu_has_mfma() -> bool:
-    if not is_hip():
+def mfma_supported_granularity(m, n, k) -> bool:
+    # todo make this gran_type matrix element type sensitive
+    for gran_type in [(32, 8), (16, 16), (4, 64)]:
+        granularity_mn, granularity_k = gran_type
+
+        if m % granularity_mn != 0 or n % granularity_mn != 0:
+            continue
+        if k % granularity_k != 0:
+            continue
+        return True
+    return False
+
+def mfma_supported(M, N, K, allow_tf32, ret_scalar_ty, target) -> bool:
+    matrix_core_version = target["matrix_core_version"]
+    if matrix_core_version not in [1, 2, 3]:
         return False
-    return True  # mfma supported in ['gfx908', 'gfx90a']
-
-
-def mfma_supported(M, N, K, allow_tf32, ret_scalar_ty) -> bool:
-    if not gpu_has_mfma():
+    if not mfma_supported_granularity(M, N ,K):
         return False
-    # TODO: Add check for configurations and types.
     return True
 
 
@@ -1190,15 +1198,26 @@ def dot(lhs: tl.tensor, rhs: tl.tensor, acc: tl.tensor, allow_tf32: bool, max_nu
 
     def assert_dtypes_valid(lhs_dtype, rhs_dtype, target):
         # Checks for non-cuda archs
+        if is_hip():
+            assert lhs.dtype == rhs.dtype or (lhs.type.scalar.is_fp8() and rhs.type.scalar.is_fp16()) or \
+                (lhs.type.scalar.is_fp16() and rhs.type.scalar.is_fp8()) or (lhs.type.scalar.is_fp8() and rhs.type.scalar.is_fp8()), \
+                f"First input ({lhs.dtype}) and second input ({rhs.dtype}) must have the same dtype!"
+            if lhs.type.scalar.is_fp8() and rhs.type.scalar.is_fp8():
+                assert lhs.type.scalar.is_fp8e4b8() or lhs.type.scalar.is_fp8e5b16() or lhs.type.scalar.is_fp8e5(),\
+                    f"Only hip fp8 or f8e5 types are accepted for both inputs of fp8"
+                assert rhs.type.scalar.is_fp8e4b8() or rhs.type.scalar.is_fp8e5b16() or rhs.type.scalar.is_fp8e5(),\
+                    f"Only hip fp8 or f8e5 types are accepted for both inputs of fp8"
+            return
         if not _is_cuda(target):
             assert lhs_dtype == rhs_dtype, f"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!"
             return
+
         # Checks for cuda arch
         if target.capability < 90:
             assert not lhs_dtype.is_fp8e4nv() and not rhs_dtype.is_fp8e4nv(
             ), "Dot op does not support fp8e4nv on CUDA arch < 90"
             if lhs_dtype.is_fp8() and rhs_dtype.is_fp8():
-                return
+              return
             assert lhs_dtype == rhs_dtype, f"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!"
         else:
             assert not lhs_dtype.is_fp8e4b15() and not rhs_dtype.is_fp8e4b15(
@@ -1232,10 +1251,31 @@ def dot(lhs: tl.tensor, rhs: tl.tensor, acc: tl.tensor, allow_tf32: bool, max_nu
     assert lhs.shape[0].value >= 16 and lhs.shape[1].value >= 16 \
         and rhs.shape[1].value >= 16, \
         f"All values in both first input shape ({lhs.shape}) and second input shape ({rhs.shape}) must be >= 16!"
+    if is_hip():
+        assert lhs.shape[0].value >= 4 and lhs.shape[1].value >= 16 \
+            and rhs.shape[1].value >= 4, \
+            f"All values in both first input shape ({lhs.shape}) and second input shape ({rhs.shape}) must be >= 4!"
+
+    # hip for now converts fp8 to fp16 for mixed input
+    if is_hip():
+        target = builder.target
+        assert "matrix_core_version" in target
+        fp8_supported = target["matrix_core_version"] == 3
+        # gfx940 data type
+        lhs_hip_fp8 = lhs.type.scalar.is_fp8e4b8() or lhs.type.scalar.is_fp8e5b16()
+        rhs_hip_fp8 = rhs.type.scalar.is_fp8e4b8() or rhs.type.scalar.is_fp8e5b16()
+        lhs_fp8 = lhs.type.scalar.is_fp8()
+        rhs_fp8 = rhs.type.scalar.is_fp8()
+        supported_fp8_dot = fp8_supported and lhs_hip_fp8 and rhs_hip_fp8
+        if (not supported_fp8_dot) and lhs_fp8:
+            lhs = cast(lhs, tl.float16, builder)
+        if (not supported_fp8_dot) and rhs_fp8:
+            rhs = cast(rhs, tl.float16, builder)
+
     if lhs.type.scalar.is_int():
         assert lhs.type.scalar == tl.int8, "only int8 supported!"
         # TODO: This is CUDA specific, check if ROCm has the same limitation
-        assert lhs.shape[1].value >= 32, "small blocks not supported!"
+        assert is_hip() or lhs.shape[1].value >= 32, "small blocks not supported!"
         _0 = builder.get_int32(0)
         ret_scalar_ty = tl.int32
     elif out_dtype.is_bf16():
@@ -1252,7 +1292,12 @@ def dot(lhs: tl.tensor, rhs: tl.tensor, acc: tl.tensor, allow_tf32: bool, max_nu
     N = rhs.type.shape[1]
 
     # Cast operands of types f16 and i8 for configurations where FMA only supported.
-    if is_hip() and not mfma_supported(M, N, lhs.type.shape[1], allow_tf32, ret_scalar_ty):
+    if is_hip() and not mfma_supported(M, N, lhs.type.shape[1], allow_tf32, ret_scalar_ty, builder.target):
+        # max_num_imprecise_acc does not yet apply to hip
+        if is_hip():
+            max_num_imprecise_acc = 0
+        if max_num_imprecise_acc is None:
+            max_num_imprecise_acc = 2**30
         ret_cast_scalar_ty = tl.float32 if lhs.type.scalar.is_int() else ret_scalar_ty
         lhs = cast(lhs, ret_cast_scalar_ty, builder)
         rhs = cast(rhs, ret_cast_scalar_ty, builder)
@@ -1261,10 +1306,17 @@ def dot(lhs: tl.tensor, rhs: tl.tensor, acc: tl.tensor, allow_tf32: bool, max_nu
         else:
             _0 = builder.create_splat(builder.get_fp32(0), [M, N])
         ret_ty = tl.block_type(ret_cast_scalar_ty, [M, N])
-        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32), ret_ty)
+        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32, max_num_imprecise_acc),
+                        ret_ty)
         return cast(ret, ret_scalar_ty, builder)
     if is_hip() and mfma_supported(M, N, lhs.type.shape[1], allow_tf32,
-                                   ret_scalar_ty) and ret_scalar_ty.primitive_bitwidth < 32:
+                                   ret_scalar_ty, builder.target) and ret_scalar_ty.primitive_bitwidth <= 32:
+        # max_num_imprecise_acc does not yet apply to hip
+        if is_hip():
+            max_num_imprecise_acc = 0
+        if max_num_imprecise_acc is None:
+            max_num_imprecise_acc = 2**30
+
         if lhs.type.scalar.is_int():
             ret_dot_scalar_ty = tl.int32
             _0 = builder.create_splat(builder.get_int32(0), [M, N])
@@ -1272,7 +1324,8 @@ def dot(lhs: tl.tensor, rhs: tl.tensor, acc: tl.tensor, allow_tf32: bool, max_nu
             ret_dot_scalar_ty = tl.float32
             _0 = builder.create_splat(builder.get_fp32(0), [M, N])
         ret_ty = tl.block_type(ret_dot_scalar_ty, [M, N])
-        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32), ret_ty)
+        ret = tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32, max_num_imprecise_acc),
+                        ret_ty)
         return cast(ret, ret_scalar_ty, builder)
     ret_ty = tl.block_type(ret_scalar_ty, [M, N])
     if acc is None:
diff --git a/third_party/amd_hip_backend b/third_party/amd_hip_backend
index d0ad70d55..50f4b5afe 160000
--- a/third_party/amd_hip_backend
+++ b/third_party/amd_hip_backend
@@ -1 +1 @@
-Subproject commit d0ad70d55df3ebe11cc80bbb364a91551e6b6248
+Subproject commit 50f4b5afe287e78c937b80dcd688390207f22b72
-- 
2.17.1

